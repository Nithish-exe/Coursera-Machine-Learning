{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "np.set_printoptions(precision=2) # reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7f494",
   "metadata": {},
   "source": [
    "You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below. Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
    "\n",
    "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e40ff471",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]]) # each list element is one house as [cost, bedrooms, floors, age]\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1bc89520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Shape: (3, 4), X Type:<class 'numpy.ndarray'>)\n",
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "y Shape: (3,), y Type:<class 'numpy.ndarray'>)\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "# the data is stored in a matrix\n",
    "print(f\"x Shape: {x_train.shape}, x Type:{type(x_train)})\")\n",
    "print(x_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1ff8e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# for the purpose of the lab, w and b are chosen initialized near optimal values\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4b443dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model for multivariate regression is a little different\n",
    "# is looks like this: fwb(x)=w1x1+...+wnxn+b where inputs w and x are vectors and elements i are being multipled in the function\n",
    "def predict_single_loop(x,w,b):\n",
    "    n = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        p_i = x[i]*w[i]\n",
    "        p = p + p_i\n",
    "        print(p) # visualize p compounding\n",
    "    p = p + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "30954ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "823.3695764\n",
      "917.13841345\n",
      "863.7780889200001\n",
      "-325.18113917999995\n",
      "f_wb shape (), prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# with this method, we can use a row from our training data and make a prediction\n",
    "x_vec = x_train[0,:] # only first vector\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "814c420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    p = np.dot(x, w) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5d17f66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194082\n"
     ]
    }
   ],
   "source": [
    "# with this method, we can use a row from our training data and make a prediction\n",
    "# because we are using the dot function we will have faster processing and shorter code\n",
    "x_vec = x_train[0,:] # only first vector\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "f_wb = predict(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1bfb8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cost function for multivariate regression is the same except for that fact that x and w are vectors and not scalars\n",
    "# this helps support prediction for multiple features\n",
    "def compute_cost(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(x[i], w) + b\n",
    "        cost = cost + (f_wb_i - y[i])**2\n",
    "    cost = cost / (2 * m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d6e8b48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904330213735e-12\n"
     ]
    }
   ],
   "source": [
    "# we can now compute and display cost using our pre-chosen optimal parameters\n",
    "# remember the cost is the closest to 0 at the optimal w and b\n",
    "cost = compute_cost(x_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "94b81df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b):\n",
    "    m,n = x.shape # number of examples and number of features(inputs)\n",
    "    dj_dw = np.zeros((n,)) # dJ/dw\n",
    "    dj_db = 0. # dJ/db\n",
    "    for i in range(m):\n",
    "        err = (np.dot(x[i], w) + b) - y[i] # find error(cost for 1 data point)  \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * x[i, j] # iterate through each element in each array in x_train and compund error for each feature(weight)\n",
    "        dj_db = dj_db + err # calculate error for all features(bias)        \n",
    "    dj_dw = dj_dw / m # average w for all given data points                       \n",
    "    dj_db = dj_db / m # average b for all given data points                             \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b4e21ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: -1.6739251122999121e-06\n",
      "dj_dw at initial w,b: \n",
      " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n"
     ]
    }
   ],
   "source": [
    "# we can now compute and display gradient for our 2d array\n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(x_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1cf71434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    J_history = [] # function history saved to see iterations, used for graphing\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters): # calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(x, y, w, b) # store gradient\n",
    "        w = w - alpha * dj_dw # run descent on w for x iterations\n",
    "        b = b - alpha * dj_db # run descent on b for x iterations\n",
    "        if i<100000: # less than 100,000 to prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w, b))\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \") \n",
    "    return w, b, J_history # return w, b, and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e6e12563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros_like(w_init) # initialize w with predetermined values\n",
    "initial_b = 0. # initialize b, default is 0\n",
    "iterations = 1000 # gradient settings from lab\n",
    "alpha = 5.0e-7 # gradient settings from lab\n",
    "w_final, b_final, J_hist = gradient_descent(x_train, y_train, initial_w, initial_b, compute_cost, compute_gradient, alpha, iterations) # notice cost is decreasing but predictions are still inaccurate\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = x_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(x_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\") # we can see here that our predictions are off inconsitently even though cost decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d64d3",
   "metadata": {},
   "source": [
    "This issue of the cost decreasing while the results are still inconsitent will be improved upon in the next lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356467ef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "15456"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
